{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Initialize Watsonx LLM\n",
    "def watsonx_llm(messages):\n",
    "    model_id = \"meta-llama/llama-3-1-8b-instruct\"\n",
    "    api_key = os.getenv(\"WX_KEY\")\n",
    "    url = os.getenv(\"WX_URL\")\n",
    "    project_id = os.getenv(\"WX_PID\")\n",
    "\n",
    "    if not all([api_key, project_id, model_id]):\n",
    "        raise Exception(\n",
    "            \"Missing required environment variables: WX_KEY, WX_URL, or WX_PID.\"\n",
    "        )\n",
    "\n",
    "    params = {\n",
    "        GenParams.DECODING_METHOD: \"sample\",\n",
    "        GenParams.MAX_NEW_TOKENS: 256,\n",
    "        GenParams.TEMPERATURE: 0.0,\n",
    "        GenParams.STOP_SEQUENCES: [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "    }\n",
    "\n",
    "    credentials = Credentials(api_key=api_key, url=url)\n",
    "    model_inference = ModelInference(\n",
    "        model_id=model_id, params=params, credentials=credentials, project_id=project_id\n",
    "    )\n",
    "    response = model_inference.chat(messages=messages)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Preprocess Documents for RAG\n",
    "loader = DirectoryLoader(\n",
    "    \"/Users/charan/VSCode/EMEA/Comarch_Telco_OSS_Assistant/data\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embedding and Vector Store\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=os.getenv(\"WX_URL\"),\n",
    "    apikey=os.getenv(\"WX_KEY\"),\n",
    "    project_id=os.getenv(\"WX_PID\"),\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    split_docs, collection_name=\"agentic-rag-chroma\", embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "\n",
    "# Define Retrieval Tool\n",
    "def get_context(question):\n",
    "    results = retriever.invoke(question)\n",
    "    return \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "# Define available actions\n",
    "known_actions = {\n",
    "    \"get_context\": get_context,\n",
    "}\n",
    "\n",
    "\n",
    "# Define ReAct Agent\n",
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        return watsonx_llm(self.messages)\n",
    "\n",
    "\n",
    "# Initialize the prompt\n",
    "prompt = \"\"\"\n",
    "You operate as a retrieval-augmented assistant with tools to fetch information from documents.\n",
    "You follow this loop:\n",
    "\n",
    "1. Thought: Describe your reasoning process.\n",
    "2. Action: Run a tool to get additional context or answer the question.\n",
    "3. Observation: Report the results of the action.\n",
    "4. Answer: Use the retrieved information and your reasoning to answer.\n",
    "\n",
    "Available actions:\n",
    "\n",
    "get_context:\n",
    "e.g., get_context: What is the definition of Generic Network Slice Template?\n",
    "Fetches context about the given question from stored documents.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Define the query function with ReAct loop\n",
    "action_re = re.compile(r\"^Action: (\\w+): (.*)$\")\n",
    "\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [action_re.match(a) for a in result.split(\"\\n\") if action_re.match(a)]\n",
    "        if actions:\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(f\"Unknown action: {action}: {action_input}\")\n",
    "            print(f\" -- running {action} {action_input}\")\n",
    "            observation = known_actions[action](action_input)\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "question = \"What is Generic Network Slice Template as defined by GSMA?\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "question = \"What is Generic Network Slice Template as defined by GSMA?\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
