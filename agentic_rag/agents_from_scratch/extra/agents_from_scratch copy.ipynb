{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Watsonx chat function\n",
    "def watsonx_llm(messages):\n",
    "    \"\"\"\n",
    "    Generate a chat completion using IBM Watsonx models.\n",
    "\n",
    "    Parameters:\n",
    "    - messages (list of dict): The messages for the chat session.\n",
    "    - model_size (int, optional): Specifies the model size. Default is 8.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated response content.\n",
    "    \"\"\"\n",
    "    model_id = f\"meta-llama/llama-3-1-8b-instruct\"\n",
    "\n",
    "    # Fetch environment variables\n",
    "    api_key = os.getenv(\"WX_KEY\")\n",
    "    url = os.getenv(\"WX_URL\")\n",
    "    project_id = os.getenv(\"WX_PID\")\n",
    "\n",
    "    if not all([api_key, project_id, model_id]):\n",
    "        raise Exception(\n",
    "            \"Missing required environment variables: WX_KEY, WX_URL, or WX_PID.\"\n",
    "        )\n",
    "\n",
    "    # Adjusted parameters\n",
    "    params = {\n",
    "        GenParams.DECODING_METHOD: \"sample\",\n",
    "        GenParams.MAX_NEW_TOKENS: 256,\n",
    "        GenParams.TEMPERATURE: 0.0,\n",
    "        GenParams.STOP_SEQUENCES: [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "    }\n",
    "\n",
    "    # Setup credentials\n",
    "    credentials = Credentials(api_key=api_key, url=url)\n",
    "\n",
    "    # Instantiate ModelInference\n",
    "    model_inference = ModelInference(\n",
    "        model_id=model_id, params=params, credentials=credentials, project_id=project_id\n",
    "    )\n",
    "\n",
    "    # Call the chat method\n",
    "    response = model_inference.chat(messages=messages)\n",
    "\n",
    "    # Extract and return the generated content\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Define the ReAct Agent class\n",
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        return watsonx_llm(self.messages)\n",
    "\n",
    "\n",
    "# Define available actions\n",
    "def calculate(what):\n",
    "    try:\n",
    "        return eval(what)\n",
    "    except Exception as e:\n",
    "        return f\"Error in calculation: {e}\"\n",
    "\n",
    "\n",
    "def average_dog_weight(name):\n",
    "    if name in \"Scottish Terrier\":\n",
    "        return \"Scottish Terriers average 20 lbs\"\n",
    "    elif name in \"Border Collie\":\n",
    "        return \"a Border Collies average weight is 37 lbs\"\n",
    "    elif name in \"Toy Poodle\":\n",
    "        return \"a toy poodles average weight is 7 lbs\"\n",
    "    else:\n",
    "        return \"An average dog weighs 50 lbs\"\n",
    "\n",
    "\n",
    "def calculator(expression):\n",
    "    \"\"\"\n",
    "    A more advanced calculator that supports complex mathematical expressions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safeguard to limit the operations\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# Add tools to known actions\n",
    "known_actions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"average_dog_weight\": average_dog_weight,\n",
    "    \"calculator\": calculator,\n",
    "}\n",
    "\n",
    "# Initialize the prompt\n",
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer.\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary.\n",
    "\n",
    "average_dog_weight:\n",
    "e.g. average_dog_weight: Collie\n",
    "Returns the average weight of a dog when given the breed.\n",
    "\n",
    "calculator:\n",
    "e.g. calculator: (5 + 10) * (2 ** 3)\n",
    "Runs a more advanced calculation and returns the result. It can handle complex mathematical expressions.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the result of (3 + 7) * 2?\n",
    "Thought: I should use the calculator tool to compute the result.\n",
    "Action: calculator: (3 + 7) * 2\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: The result is 20\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: The result of (3 + 7) * 2 is 20.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Define the query function with ReAct loop\n",
    "action_re = re.compile(r\"^Action: (\\w+): (.*)$\")\n",
    "\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [action_re.match(a) for a in result.split(\"\\n\") if action_re.match(a)]\n",
    "        if actions:\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(f\"Unknown action: {action}: {action_input}\")\n",
    "            print(f\" -- running {action} {action_input}\")\n",
    "            observation = known_actions[action](action_input)\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            return\n",
    "\n",
    "\n",
    "# Example usage\n",
    "question = \"I have 2 dogs, a Border Collie and a Scottish Terrier. What is their combined weight?\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Initialize Watsonx LLM\n",
    "def watsonx_llm(messages):\n",
    "    model_id = \"meta-llama/llama-3-1-8b-instruct\"\n",
    "    api_key = os.getenv(\"WX_KEY\")\n",
    "    url = os.getenv(\"WX_URL\")\n",
    "    project_id = os.getenv(\"WX_PID\")\n",
    "\n",
    "    if not all([api_key, project_id, model_id]):\n",
    "        raise Exception(\n",
    "            \"Missing required environment variables: WX_KEY, WX_URL, or WX_PID.\"\n",
    "        )\n",
    "\n",
    "    params = {\n",
    "        GenParams.DECODING_METHOD: \"sample\",\n",
    "        GenParams.MAX_NEW_TOKENS: 256,\n",
    "        GenParams.TEMPERATURE: 0.0,\n",
    "        GenParams.STOP_SEQUENCES: [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "    }\n",
    "\n",
    "    credentials = Credentials(api_key=api_key, url=url)\n",
    "    model_inference = ModelInference(\n",
    "        model_id=model_id, params=params, credentials=credentials, project_id=project_id\n",
    "    )\n",
    "    response = model_inference.chat(messages=messages)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Preprocess Documents for RAG\n",
    "loader = DirectoryLoader(\n",
    "    \"/Users/charan/VSCode/EMEA/Comarch_Telco_OSS_Assistant/data\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embedding and Vector Store\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=os.getenv(\"WX_URL\"),\n",
    "    apikey=os.getenv(\"WX_KEY\"),\n",
    "    project_id=os.getenv(\"WX_PID\"),\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    split_docs, collection_name=\"agentic-rag-chroma\", embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "\n",
    "# Define Retrieval Tool\n",
    "def get_context(question):\n",
    "    results = retriever.invoke(question)\n",
    "    return \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "# Define available actions\n",
    "known_actions = {\n",
    "    \"get_context\": get_context,\n",
    "}\n",
    "\n",
    "\n",
    "# Define ReAct Agent\n",
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        return watsonx_llm(self.messages)\n",
    "\n",
    "\n",
    "# Initialize the prompt\n",
    "prompt = \"\"\"\n",
    "You operate as a retrieval-augmented assistant with tools to fetch information from documents.\n",
    "You follow this loop:\n",
    "\n",
    "1. Thought: Describe your reasoning process.\n",
    "2. Action: Run a tool to get additional context or answer the question.\n",
    "3. Observation: Report the results of the action.\n",
    "4. Answer: Use the retrieved information and your reasoning to answer.\n",
    "\n",
    "Available actions:\n",
    "\n",
    "get_context:\n",
    "e.g., get_context: What is the definition of Generic Network Slice Template?\n",
    "Fetches context about the given question from stored documents.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Define the query function with ReAct loop\n",
    "action_re = re.compile(r\"^Action: (\\w+): (.*)$\")\n",
    "\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [action_re.match(a) for a in result.split(\"\\n\") if action_re.match(a)]\n",
    "        if actions:\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(f\"Unknown action: {action}: {action_input}\")\n",
    "            print(f\" -- running {action} {action_input}\")\n",
    "            observation = known_actions[action](action_input)\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "question = \"What is Generic Network Slice Template as defined by GSMA?\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "question = \"What is Generic Network Slice Template as defined by GSMA?\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
