{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ---- Environment Loader ----\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "def load_env():\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "# ---- Watsonx Model Inference Classes ----\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Chat Completion Function (Llama 3.2 Vision) ----\n",
    "def llama32(messages, model_size=11):\n",
    "    \"\"\"\n",
    "    Generate a chat completion using IBM Watsonx Llama 3.2 Vision Instruct.\n",
    "\n",
    "    Parameters:\n",
    "    - messages (list of dict): The messages for the chat session.\n",
    "    - model_size (int, optional): Specifies the model size. Default is 11.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated response content.\n",
    "    \"\"\"\n",
    "    load_env()  # Load environment variables from .env\n",
    "    model_id = f\"meta-llama/llama-3-2-{model_size}b-vision-instruct\"\n",
    "\n",
    "    # Fetch environment variables\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    "    url = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n",
    "    project_id = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "    if not all([api_key, project_id, model_id]):\n",
    "        raise Exception(\n",
    "            \"Missing one or more required environment variables:\\n\"\n",
    "            \"  - WATSONX_API_KEY\\n\"\n",
    "            \"  - WATSONX_URL\\n\"\n",
    "            \"  - WATSONX_PROJECT_ID\\n\"\n",
    "            f\"  - model_id={model_id} from code\"\n",
    "        )\n",
    "\n",
    "    # Define generation parameters\n",
    "    # params = {\n",
    "    #     \"max_new_tokens\": 4096,  # or adjust to your preference\n",
    "    #     \"temperature\": 0.0,      # adjust if you want more creative answers\n",
    "    #     \"stop_sequences\": [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "    # }\n",
    "\n",
    "    credentials = Credentials(api_key=api_key, url=url)\n",
    "    model_inference = ModelInference(\n",
    "        model_id=model_id, credentials=credentials, project_id=project_id\n",
    "    )\n",
    "\n",
    "    print(\"Here 3\")\n",
    "\n",
    "    # Call the chat method\n",
    "    response = model_inference.chat(messages=messages)\n",
    "\n",
    "    print(\"Here 4\")\n",
    "\n",
    "    # Extract and return the generated content\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper to Display an Image in Notebook/Script ----\n",
    "def disp_image(address):\n",
    "    \"\"\"\n",
    "    Display an image from either a URL or a local file path.\n",
    "    \"\"\"\n",
    "    if address.startswith(\"http://\") or address.startswith(\"https://\"):\n",
    "        response = requests.get(address)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(address)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper to Encode a Local Image in Base64 ----\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- MAIN SCRIPT EXAMPLE -----------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load any environment variables (API keys, project IDs, etc.)\n",
    "    load_env()\n",
    "\n",
    "    # 2) Specify your local image path\n",
    "    local_image_path = \"image.jpeg\"  # Change if needed\n",
    "\n",
    "    # 3) Encode local image to Base64\n",
    "    base64_image = encode_image(local_image_path)\n",
    "\n",
    "    # 4) Display the local image (optional, for notebooks or interactive sessions)\n",
    "    # disp_image(local_image_path)\n",
    "\n",
    "    # 5) Prepare a chat-style prompt with text + image\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe this local image in one sentence.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    print(\"Here 1\")\n",
    "    # 6) Call llama32 to get an answer about the image\n",
    "    response = llama32(messages, model_size=11)\n",
    "    print(\"Here 2\")\n",
    "\n",
    "    # 7) Print the result\n",
    "    print(\"Model Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
